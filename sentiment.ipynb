{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f583e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Import necessary libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer # For stemming words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB # For the sentiment classification model\n",
    "from sklearn.metrics import accuracy_score, classification_report # For evaluating the model\n",
    "import pickle\n",
    "import re # For regular expressions in text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a49eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and downloading NLTK data...\n",
      "NLTK 'vader_lexicon' downloaded.\n",
      "NLTK data check complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Download NLTK data (if not already downloaded) ---\n",
    "# These are essential for text preprocessing (stopwords) and the VADER lexicon check (if needed by Flask)\n",
    "print(\"Checking and downloading NLTK data...\")\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    print(\"NLTK 'stopwords' downloaded.\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('sentiment/vader_lexicon')\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon')\n",
    "    print(\"NLTK 'vader_lexicon' downloaded.\")\n",
    "print(\"NLTK data check complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc18049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully. Shape: (50000, 2)\n",
      "First 5 rows of the dataset:\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Load the dataset ---\n",
    "# This assumes 'reviews.csv' is in the same directory as the notebook.\n",
    "# It should have 'review' and 'sentiment' columns.\n",
    "try:\n",
    "    dataset = pd.read_csv('reviews.csv')\n",
    "    print(f\"Dataset loaded successfully. Shape: {dataset.shape}\")\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(dataset.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'reviews.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    # Exit or handle gracefully if the dataset isn't found\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe42ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. Prepare text preprocessing tools ---\n",
    "# Set of English stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "# Initialize Porter Stemmer for reducing words to their root form\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7aadf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. Define text preprocessing function ---\n",
    "# This function will clean and transform each review comment.\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters and replace with space\n",
    "    review = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    review = review.lower()\n",
    "    # Split into individual words\n",
    "    review = review.split()\n",
    "    # Apply stemming and remove stopwords\n",
    "    # Only stem if the word is not a stopword\n",
    "    review = [ps.stem(word) for word in review if not word in stopset]\n",
    "    # Join words back into a single string\n",
    "    review = ' '.join(review)\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae7bce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing movie reviews... This might take a moment.\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 6. Apply preprocessing to the 'review' column ---\n",
    "print(\"\\nPreprocessing movie reviews... This might take a moment.\")\n",
    "# Apply the preprocessing function to each review in the 'review' column\n",
    "# Using .apply() with a lambda for cleaner syntax\n",
    "corpus = dataset['review'].apply(preprocess_text)\n",
    "print(\"Preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a8e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "TF-IDF features created. Shape: (50000, 68929)\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Convert text data to TF-IDF features ---\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer\n",
    "# It assigns a weight to each word, indicating its importance in a document relative to the corpus.\n",
    "# use_idf: Enable inverse-document frequency reweighting.\n",
    "# lowercase: Convert all characters to lowercase before tokenizing.\n",
    "# strip_accents: Remove accents during the preprocessing step.\n",
    "# stop_words: Remove common English stop words.\n",
    "print(\"Creating TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=list(stopset)) # <-- Changed here\n",
    "X = vectorizer.fit_transform(corpus) # Fit and transform the preprocessed text data\n",
    "print(f\"TF-IDF features created. Shape: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "361d759a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorizer saved as 'tranform.pkl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 8. Save the trained TF-IDF Vectorizer ---\n",
    "# The vectorizer needs to be saved so it can be used later in the Flask app\n",
    "# to transform new movie reviews in the same way the training data was transformed.\n",
    "filename_vectorizer = 'tranform.pkl'\n",
    "with open(filename_vectorizer, 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "print(f\"TF-IDF Vectorizer saved as '{filename_vectorizer}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc09cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment labels mapped. First 5 labels: [1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 9. Prepare labels (target variable) ---\n",
    "# Map 'positive' to 1 and 'negative' to 0 for numerical classification\n",
    "y = dataset['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "print(f\"Sentiment labels mapped. First 5 labels: {y.head().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ed19fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and testing sets...\n",
      "Training set shape (X_train, y_train): (40000, 68929), (40000,)\n",
      "Test set shape (X_test, y_test): (10000, 68929), (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 10. Split data into training and testing sets ---\n",
    "# Split the data to evaluate the model's performance on unseen data.\n",
    "# test_size=0.20: 20% of data for testing, 80% for training.\n",
    "# random_state=42: Ensures reproducibility of the split.\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(f\"Training set shape (X_train, y_train): {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set shape (X_test, y_test): {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f121e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Multinomial Naive Bayes classifier...\n",
      "Classifier training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 11. Train the Naive Bayes Classifier ---\n",
    "# Multinomial Naive Bayes is a common and effective model for text classification.\n",
    "print(\"\\nTraining Multinomial Naive Bayes classifier...\")\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train) # Train the classifier on the training data\n",
    "print(\"Classifier training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b1d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test set...\n",
      "Accuracy on test set: 86.16%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86      4961\n",
      "           1       0.87      0.85      0.86      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 12. Evaluate the model on the test set ---\n",
    "print(\"\\nEvaluating model on test set...\")\n",
    "y_pred_test = clf.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test) * 100\n",
    "print(f\"Accuracy on test set: {accuracy_test:.2f}%\")\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "788eae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Re-training classifier on the full dataset...\n",
      "Full data classifier training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 13. Re-train the classifier on the full dataset ---\n",
    "# It's common practice to train the final model on the entire dataset\n",
    "# after evaluating it, to maximize the data used for learning.\n",
    "print(\"\\nRe-training classifier on the full dataset...\")\n",
    "clf_full_data = MultinomialNB()\n",
    "clf_full_data.fit(X, y) # Train on all TF-IDF features and labels\n",
    "print(\"Full data classifier training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d63ced7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP model saved as 'nlp_model.pkl'\n",
      "\n",
      "Sentiment analysis model training and saving process finished successfully!\n",
      "You can now find 'nlp_model.pkl' and 'tranform.pkl' in your project directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 14. Save the trained Naive Bayes Classifier ---\n",
    "# This is the model that the Flask app will load to make sentiment predictions.\n",
    "filename_model = 'nlp_model.pkl'\n",
    "with open(filename_model, 'wb') as file:\n",
    "    pickle.dump(clf_full_data, file)\n",
    "print(f\"NLP model saved as '{filename_model}'\")\n",
    "\n",
    "print(\"\\nSentiment analysis model training and saving process finished successfully!\")\n",
    "print(\"You can now find 'nlp_model.pkl' and 'tranform.pkl' in your project directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617c55d-49e5-4d13-a1fb-d9029d8f8850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92b4db-830c-49ed-b26c-cd4a3a519a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
